{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r test\n",
    "\n",
    "# Take the name of the different classes (e.g. '0', '1', '2', ...)\n",
    "classes = test.groupby(['label'])['label'].unique().index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test = test.copy()\n",
    "\n",
    "feature_1= 'x'\n",
    "feature_2='y'\n",
    "\n",
    "models = []\n",
    "for n_class in classes: # Can be thrown an expection beacuse this is the labels of the test set and not of the models (train set)\n",
    "    model = pickle.load(open('models_1_2/'+str(n_class)+'hmm_model.pkl', 'rb'))\n",
    "    models.append(model)\n",
    "\n",
    "#log_likelihoods = pd.DataFrame()\n",
    "#log_likelihoods['predicted_class'] = classes\n",
    "#log_like_max = []\n",
    "#top_3_loglikelihood = []\n",
    "top_3_predicted_classes = []\n",
    "predicted_class = []\n",
    "#is_predicted = []\n",
    "true_label=[]\n",
    "\n",
    "for i in range(len(test)):\n",
    "    angle_1 = test[feature_1].iloc[i]\n",
    "    angle_2 = test[feature_2].iloc[i]\n",
    "    X=np.array([angle_1,angle_2]).T\n",
    "    true_label.append(test['label'].iloc[i])\n",
    "    list_log_likelihoods = []\n",
    "    for model in models:\n",
    "        \n",
    "        list_log_likelihoods.append(model.score(X,lengths=len(X)))\n",
    "        pass\n",
    "\n",
    "    predicted_class.append(np.argmax(list_log_likelihoods))\n",
    "\n",
    "    top_3_values = sorted(list_log_likelihoods)[-3:]\n",
    "    top_3_predicted_classes.append([i for i, x in enumerate(list_log_likelihoods) if x == top_3_values[0] or x == top_3_values[1] or x == top_3_values[2]])\n",
    "    \n",
    "# model_0.score_samples(angles_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 1, 9, 3, 0, 8, 0, 4, 5, 0, 8, 4, 1, 2, 6, 4, 4, 4, 1, 2, 8, 0, 5, 0, 5, 2, 0, 0, 5, 2, 7, 5, 2, 0, 0, 4, 2, 2, 2, 9, 3, 2, 9, 5, 0, 7, 3, 5, 8, 6, 9, 4, 6, 3, 3, 1, 0, 1, 6, 8, 7, 4, 6, 9, 9, 4, 1, 0, 0, 4, 5, 3, 6, 1, 6, 7, 4, 2, 9, 0, 7, 9, 8, 7, 2, 8, 7, 6, 3, 8, 4, 1, 0, 4, 1, 9, 5, 2, 8, 7, 3, 0, 6, 8, 5, 5, 1, 1, 1, 7, 8, 1, 5, 8, 6, 6, 4, 3, 6, 4, 5, 5, 8, 5, 0, 9, 6, 3, 0, 6, 0, 2, 9, 9, 7, 8, 5, 1, 6, 9, 8, 3, 5, 4, 1, 0, 7, 5, 8, 7, 4, 6, 5, 1, 8, 8, 5, 0, 2, 7, 8, 6, 3, 4, 4, 4, 8, 8, 0, 4, 4, 9, 0, 8, 6, 4, 0, 9, 8, 0, 6, 1, 4, 1, 8, 2, 3, 7, 2, 0, 5, 5, 2, 5, 8, 1, 5, 6, 0, 7, 6, 6, 0, 1, 1, 3, 7, 8, 5, 3, 6, 4, 0, 4, 3, 7, 4, 8, 0, 3, 0, 0, 3, 2, 5, 3, 8, 8, 6, 0, 1, 4, 1, 2, 6, 8, 7, 1, 0, 8, 0, 5, 7, 2, 1, 2, 5, 6]\n",
      "[4, 1, 9, 3, 5, 8, 0, 4, 6, 0, 8, 4, 7, 2, 6, 4, 7, 3, 1, 2, 8, 0, 9, 0, 5, 3, 6, 4, 5, 2, 4, 5, 2, 0, 0, 4, 2, 1, 2, 9, 3, 2, 9, 3, 1, 7, 5, 5, 8, 6, 7, 4, 5, 3, 3, 1, 0, 9, 6, 8, 9, 0, 6, 9, 9, 8, 1, 0, 2, 4, 5, 2, 6, 9, 3, 4, 4, 2, 6, 0, 9, 9, 8, 7, 2, 0, 7, 5, 5, 8, 5, 1, 8, 4, 1, 9, 8, 2, 9, 7, 5, 0, 0, 8, 5, 5, 2, 1, 1, 1, 1, 7, 5, 5, 2, 3, 4, 3, 6, 4, 5, 9, 3, 6, 0, 7, 6, 2, 0, 6, 0, 2, 7, 7, 8, 8, 0, 1, 6, 9, 8, 3, 9, 4, 7, 0, 7, 3, 8, 3, 4, 6, 7, 1, 7, 8, 4, 0, 6, 7, 9, 6, 9, 4, 4, 0, 8, 8, 0, 4, 4, 9, 8, 5, 6, 0, 0, 9, 8, 0, 9, 1, 4, 2, 0, 5, 3, 0, 3, 0, 5, 9, 2, 5, 8, 1, 5, 6, 9, 7, 6, 6, 0, 1, 1, 3, 1, 8, 5, 3, 6, 0, 0, 4, 3, 7, 0, 7, 0, 3, 9, 6, 2, 2, 5, 3, 4, 9, 6, 7, 1, 4, 1, 2, 6, 3, 9, 1, 0, 8, 0, 8, 7, 2, 1, 2, 5, 6]\n",
      "248\n",
      "248\n"
     ]
    }
   ],
   "source": [
    "print(predicted_class)\n",
    "print(true_label)\n",
    "\n",
    "print(len(predicted_class))\n",
    "print(len(true_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import contingency_matrix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix=contingency_matrix(true_label, predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24,  0,  0,  0,  5,  1,  1,  1,  2,  0],\n",
       "       [ 1, 18,  1,  0,  0,  0,  0,  2,  1,  0],\n",
       "       [ 1,  2, 16,  3,  0,  0,  1,  0,  0,  0],\n",
       "       [ 0,  0,  2, 12,  1,  2,  2,  1,  2,  0],\n",
       "       [ 1,  0,  0,  0, 20,  1,  0,  2,  1,  0],\n",
       "       [ 1,  0,  1,  3,  1, 15,  2,  0,  2,  0],\n",
       "       [ 2,  0,  1,  0,  0,  2, 19,  0,  0,  1],\n",
       "       [ 1,  3,  0,  0,  1,  1,  0,  9,  2,  4],\n",
       "       [ 2,  0,  0,  0,  1,  2,  0,  1, 18,  0],\n",
       "       [ 2,  2,  0,  1,  0,  4,  1,  3,  3, 10]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6491935483870968\n"
     ]
    }
   ],
   "source": [
    "accuracy=np.sum(np.diag(matrix))/np.sum(matrix)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy top 3:  0.8467741935483871\n"
     ]
    }
   ],
   "source": [
    "# Calculate top 3 accuracy\n",
    "sum_top_3 = 0\n",
    "for i in range(len(top_3_predicted_classes)):\n",
    "    if true_label[i] in top_3_predicted_classes[i]:\n",
    "        sum_top_3 += 1\n",
    "\n",
    "accuracy_top_3 = sum_top_3/len(predicted_test[\"label\"])\n",
    "print(\"Accuracy top 3: \", accuracy_top_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f162362b51a040b66da23130964be17c9a4290e709b2d7f75b268249f5b26fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
